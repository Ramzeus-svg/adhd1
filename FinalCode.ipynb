{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramzeus-svg/adhd1/blob/main/FinalCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1234)\n",
        "from functools import reduce\n",
        "import math as m\n",
        "\n",
        "import scipy.io\n",
        "#import theano\n",
        "#import theano.tensor as T\n",
        "\n",
        "from scipy.interpolate import griddata\n",
        "from sklearn.preprocessing import scale\n",
        "#from utils import augment_EEG, cart2sph, pol2cart\n",
        "\n",
        "#import lasagne\n",
        "#from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
        "#from lasagne.layers import Conv2DLayer, MaxPool2DLayer, InputLayer\n",
        "#from lasagne.layers import DenseLayer, ElemwiseMergeLayer, FlattenLayer\n",
        "#from lasagne.layers import ConcatLayer, ReshapeLayer, get_output_shape\n",
        "#from lasagne.layers import Conv1DLayer, DimshuffleLayer, LSTMLayer, SliceLayer\n",
        "\n",
        "\n",
        "def azim_proj(pos):\n",
        "    \"\"\"\n",
        "    Computes the Azimuthal Equidistant Projection of input point in 3D Cartesian Coordinates.\n",
        "    Imagine a plane being placed against (tangent to) a globe. If\n",
        "    a light source inside the globe projects the graticule onto\n",
        "    the plane the result would be a planar, or azimuthal, map\n",
        "    projection.\n",
        "    :param pos: position in 3D Cartesian coordinates\n",
        "    :return: projected coordinates using Azimuthal Equidistant Projection\n",
        "    \"\"\"\n",
        "    [r, elev, az] = cart2sph(pos[0], pos[1], pos[2])\n",
        "    return pol2cart(az, m.pi / 2 - elev)\n",
        "\n",
        "\n",
        "def gen_images(locs, features, n_gridpoints, normalize=True,\n",
        "               augment=False, pca=False, std_mult=0.1, n_components=2, edgeless=False):\n",
        "    \"\"\"\n",
        "    Generates EEG images given electrode locations in 2D space and multiple feature values for each electrode\n",
        "    :param locs: An array with shape [n_electrodes, 2] containing X, Y\n",
        "                        coordinates for each electrode.\n",
        "    :param features: Feature matrix as [n_samples, n_features]\n",
        "                                Features are as columns.\n",
        "                                Features corresponding to each frequency band are concatenated.\n",
        "                                (alpha1, alpha2, ..., beta1, beta2,...)\n",
        "    :param n_gridpoints: Number of pixels in the output images\n",
        "    :param normalize:   Flag for whether to normalize each band over all samples\n",
        "    :param augment:     Flag for generating augmented images\n",
        "    :param pca:         Flag for PCA based data augmentation\n",
        "    :param std_mult     Multiplier for std of added noise\n",
        "    :param n_components: Number of components in PCA to retain for augmentation\n",
        "    :param edgeless:    If True generates edgeless images by adding artificial channels\n",
        "                        at four corners of the image with value = 0 (default=False).\n",
        "    :return:            Tensor of size [samples, colors, W, H] containing generated\n",
        "                        images.\n",
        "    \"\"\"\n",
        "    feat_array_temp = []\n",
        "    nElectrodes = locs.shape[0]     # Number of electrodes\n",
        "    # Test whether the feature vector length is divisible by number of electrodes\n",
        "    assert features.shape[1] % nElectrodes == 0\n",
        "    n_colors = features.shape[1] // nElectrodes\n",
        "    for c in range(int(n_colors)):\n",
        "        feat_array_temp.append(features[:, c * nElectrodes : nElectrodes * (c+1)])\n",
        "    if augment:\n",
        "        if pca:\n",
        "            for c in range(n_colors):\n",
        "                feat_array_temp[c] = augment_EEG(feat_array_temp[c], std_mult, pca=True, n_components=n_components)\n",
        "        else:\n",
        "            for c in range(n_colors):\n",
        "                feat_array_temp[c] = augment_EEG(feat_array_temp[c], std_mult, pca=False, n_components=n_components)\n",
        "    nSamples = features.shape[0]\n",
        "    # Interpolate the values\n",
        "    grid_x, grid_y = np.mgrid[\n",
        "                     min(locs[:, 0]):max(locs[:, 0]):n_gridpoints*1j,\n",
        "                     min(locs[:, 1]):max(locs[:, 1]):n_gridpoints*1j\n",
        "                     ]\n",
        "    temp_interp = []\n",
        "    for c in range(n_colors):\n",
        "        temp_interp.append(np.zeros([nSamples, n_gridpoints, n_gridpoints]))\n",
        "    # Generate edgeless images\n",
        "    if edgeless:\n",
        "        min_x, min_y = np.min(locs, axis=0)\n",
        "        max_x, max_y = np.max(locs, axis=0)\n",
        "        locs = np.append(locs, np.array([[min_x, min_y], [min_x, max_y],[max_x, min_y],[max_x, max_y]]),axis=0)\n",
        "        for c in range(n_colors):\n",
        "            feat_array_temp[c] = np.append(feat_array_temp[c], np.zeros((nSamples, 4)), axis=1)\n",
        "    # Interpolating\n",
        "    for i in range(nSamples):\n",
        "        for c in range(n_colors):\n",
        "            temp_interp[c][i, :, :] = griddata(locs, feat_array_temp[c][i, :], (grid_x, grid_y),\n",
        "                                    method='cubic', fill_value=np.nan)\n",
        "        print('Interpolating {0}/{1}\\r'.format(i+1, nSamples), end='\\r')\n",
        "    # Normalizing\n",
        "    for c in range(n_colors):\n",
        "        if normalize:\n",
        "            temp_interp[c][~np.isnan(temp_interp[c])] = \\\n",
        "                scale(temp_interp[c][~np.isnan(temp_interp[c])])\n",
        "        temp_interp[c] = np.nan_to_num(temp_interp[c])\n",
        "    return np.swapaxes(np.asarray(temp_interp), 0, 1)     # swap axes to have [samples, colors, W, H]\n",
        "\n",
        "\n",
        "def build_cnn(input_var=None, w_init=None, n_layers=(4, 2, 1), n_filters_first=32, imsize=32, n_colors=3):\n",
        "    \"\"\"\n",
        "    Builds a VGG style CNN network followed by a fully-connected layer and a softmax layer.\n",
        "    Stacks are separated by a maxpool layer. Number of kernels in each layer is twice\n",
        "    the number in previous stack.\n",
        "    input_var: Theano variable for input to the network\n",
        "    outputs: pointer to the output of the last layer of network (softmax)\n",
        "    :param input_var: theano variable as input to the network\n",
        "    :param w_init: Initial weight values\n",
        "    :param n_layers: number of layers in each stack. An array of integers with each\n",
        "                    value corresponding to the number of layers in each stack.\n",
        "                    (e.g. [4, 2, 1] == 3 stacks with 4, 2, and 1 layers in each.\n",
        "    :param n_filters_first: number of filters in the first layer\n",
        "    :param imSize: Size of the image\n",
        "    :param n_colors: Number of color channels (depth)\n",
        "    :return: a pointer to the output of last layer\n",
        "    \"\"\"\n",
        "    weights = []        # Keeps the weights for all layers\n",
        "    count = 0\n",
        "    # If no initial weight is given, initialize with GlorotUniform\n",
        "    if w_init is None:\n",
        "        w_init = [lasagne.init.GlorotUniform()] * sum(n_layers)\n",
        "    # Input layer\n",
        "    network = InputLayer(shape=(None, n_colors, imsize, imsize),\n",
        "                                        input_var=input_var)\n",
        "    for i, s in enumerate(n_layers):\n",
        "        for l in range(s):\n",
        "            network = Conv2DLayer(network, num_filters=n_filters_first * (2 ** i), filter_size=(3, 3),\n",
        "                          W=w_init[count], pad='same')\n",
        "            count += 1\n",
        "            weights.append(network.W)\n",
        "        network = MaxPool2DLayer(network, pool_size=(2, 2))\n",
        "    return network, weights\n",
        "\n",
        "\n",
        "def build_convpool_max(input_vars, nb_classes, imsize=32, n_colors=3, n_timewin=3):\n",
        "    \"\"\"\n",
        "    Builds the complete network with maxpooling layer in time.\n",
        "    :param input_vars: list of EEG images (one image per time window)\n",
        "    :param nb_classes: number of classes\n",
        "    :param imsize: size of the input image (assumes a square input)\n",
        "    :param n_colors: number of color channels in the image\n",
        "    :param n_timewin: number of time windows in the snippet\n",
        "    :return: a pointer to the output of last layer\n",
        "    \"\"\"\n",
        "    convnets = []\n",
        "    w_init = None\n",
        "    # Build 7 parallel CNNs with shared weights\n",
        "    for i in range(n_timewin):\n",
        "        if i == 0:\n",
        "            convnet, w_init = build_cnn(input_vars[i], imsize=imsize, n_colors=n_colors)\n",
        "        else:\n",
        "            convnet, _ = build_cnn(input_vars[i], w_init=w_init, imsize=imsize, n_colors=n_colors)\n",
        "        convnets.append(convnet)\n",
        "    # convpooling using Max pooling over frames\n",
        "    convpool = ElemwiseMergeLayer(convnets, theano.tensor.maximum)\n",
        "    # A fully-connected layer of 512 units with 50% dropout on its inputs:\n",
        "    convpool = DenseLayer(lasagne.layers.dropout(convpool, p=.5),\n",
        "            num_units=512, nonlinearity=lasagne.nonlinearities.rectify)\n",
        "    # And, finally, the output layer with 50% dropout on its inputs:\n",
        "    convpool = lasagne.layers.DenseLayer(lasagne.layers.dropout(convpool, p=.5),\n",
        "            num_units=nb_classes, nonlinearity=lasagne.nonlinearities.softmax)\n",
        "    return convpool\n",
        "\n",
        "\n",
        "def build_convpool_conv1d(input_vars, nb_classes, imsize=32, n_colors=3, n_timewin=3):\n",
        "    \"\"\"\n",
        "    Builds the complete network with 1D-conv layer to integrate time from sequences of EEG images.\n",
        "    :param input_vars: list of EEG images (one image per time window)\n",
        "    :param nb_classes: number of classes\n",
        "    :param imsize: size of the input image (assumes a square input)\n",
        "    :param n_colors: number of color channels in the image\n",
        "    :param n_timewin: number of time windows in the snippet\n",
        "    :return: a pointer to the output of last layer\n",
        "    \"\"\"\n",
        "    convnets = []\n",
        "    w_init = None\n",
        "    # Build 7 parallel CNNs with shared weights\n",
        "    for i in range(n_timewin):\n",
        "        if i == 0:\n",
        "            convnet, w_init = build_cnn(input_vars[i], imsize=imsize, n_colors=n_colors)\n",
        "        else:\n",
        "            convnet, _ = build_cnn(input_vars[i], w_init=w_init, imsize=imsize, n_colors=n_colors)\n",
        "        convnets.append(FlattenLayer(convnet))\n",
        "    # at this point convnets shape is [numTimeWin][n_samples, features]\n",
        "    # we want the shape to be [n_samples, features, numTimeWin]\n",
        "    convpool = ConcatLayer(convnets)\n",
        "    convpool = ReshapeLayer(convpool, ([0], n_timewin, get_output_shape(convnets[0])[1]))\n",
        "    convpool = DimshuffleLayer(convpool, (0, 2, 1))\n",
        "    # input to 1D convlayer should be in (batch_size, num_input_channels, input_length)\n",
        "    convpool = Conv1DLayer(convpool, 64, 3)\n",
        "    # A fully-connected layer of 512 units with 50% dropout on its inputs:\n",
        "    convpool = DenseLayer(lasagne.layers.dropout(convpool, p=.5),\n",
        "            num_units=512, nonlinearity=lasagne.nonlinearities.rectify)\n",
        "    # And, finally, the output layer with 50% dropout on its inputs:\n",
        "    convpool = DenseLayer(lasagne.layers.dropout(convpool, p=.5),\n",
        "            num_units=nb_classes, nonlinearity=lasagne.nonlinearities.softmax)\n",
        "    return convpool\n",
        "\n",
        "\n",
        "def build_convpool_lstm(input_vars, nb_classes, grad_clip=110, imsize=32, n_colors=3, n_timewin=3):\n",
        "    \"\"\"\n",
        "    Builds the complete network with LSTM layer to integrate time from sequences of EEG images.\n",
        "    :param input_vars: list of EEG images (one image per time window)\n",
        "    :param nb_classes: number of classes\n",
        "    :param grad_clip:  the gradient messages are clipped to the given value during\n",
        "                        the backward pass.\n",
        "    :param imsize: size of the input image (assumes a square input)\n",
        "    :param n_colors: number of color channels in the image\n",
        "    :param n_timewin: number of time windows in the snippet\n",
        "    :return: a pointer to the output of last layer\n",
        "    \"\"\"\n",
        "    convnets = []\n",
        "    w_init = None\n",
        "    # Build 7 parallel CNNs with shared weights\n",
        "    for i in range(n_timewin):\n",
        "        if i == 0:\n",
        "            convnet, w_init = build_cnn(input_vars[i], imsize=imsize, n_colors=n_colors)\n",
        "        else:\n",
        "            convnet, _ = build_cnn(input_vars[i], w_init=w_init, imsize=imsize, n_colors=n_colors)\n",
        "        convnets.append(FlattenLayer(convnet))\n",
        "    # at this point convnets shape is [numTimeWin][n_samples, features]\n",
        "    # we want the shape to be [n_samples, features, numTimeWin]\n",
        "    convpool = ConcatLayer(convnets)\n",
        "    convpool = ReshapeLayer(convpool, ([0], n_timewin, get_output_shape(convnets[0])[1]))\n",
        "    # Input to LSTM should have the shape as (batch size, SEQ_LENGTH, num_features)\n",
        "    convpool = LSTMLayer(convpool, num_units=128, grad_clipping=grad_clip,\n",
        "        nonlinearity=lasagne.nonlinearities.tanh)\n",
        "    # We only need the final prediction, we isolate that quantity and feed it\n",
        "    # to the next layer.\n",
        "    convpool = SliceLayer(convpool, -1, 1)      # Selecting the last prediction\n",
        "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
        "    convpool = DenseLayer(lasagne.layers.dropout(convpool, p=.5),\n",
        "            num_units=256, nonlinearity=lasagne.nonlinearities.rectify)\n",
        "    # And, finally, the output layer with 50% dropout on its inputs:\n",
        "    convpool = DenseLayer(lasagne.layers.dropout(convpool, p=.5),\n",
        "            num_units=nb_classes, nonlinearity=lasagne.nonlinearities.softmax)\n",
        "    return convpool\n",
        "\n",
        "\n",
        "def build_convpool_mix(input_vars, nb_classes, grad_clip=110, imsize=32, n_colors=3, n_timewin=3):\n",
        "    \"\"\"\n",
        "    Builds the complete network with LSTM and 1D-conv layers combined\n",
        "    :param input_vars: list of EEG images (one image per time window)\n",
        "    :param nb_classes: number of classes\n",
        "    :param grad_clip:  the gradient messages are clipped to the given value during\n",
        "                        the backward pass.\n",
        "    :param imsize: size of the input image (assumes a square input)\n",
        "    :param n_colors: number of color channels in the image\n",
        "    :param n_timewin: number of time windows in the snippet\n",
        "    :return: a pointer to the output of last layer\n",
        "    \"\"\"\n",
        "    convnets = []\n",
        "    w_init = None\n",
        "    # Build 7 parallel CNNs with shared weights\n",
        "    for i in range(n_timewin):\n",
        "        if i == 0:\n",
        "            convnet, w_init = build_cnn(input_vars[i], imsize=imsize, n_colors=n_colors)\n",
        "        else:\n",
        "            convnet, _ = build_cnn(input_vars[i], w_init=w_init, imsize=imsize, n_colors=n_colors)\n",
        "        convnets.append(FlattenLayer(convnet))\n",
        "    # at this point convnets shape is [numTimeWin][n_samples, features]\n",
        "    # we want the shape to be [n_samples, features, numTimeWin]\n",
        "    convpool = ConcatLayer(convnets)\n",
        "    convpool = ReshapeLayer(convpool, ([0], n_timewin, get_output_shape(convnets[0])[1]))\n",
        "    reformConvpool = DimshuffleLayer(convpool, (0, 2, 1))\n",
        "    # input to 1D convlayer should be in (batch_size, num_input_channels, input_length)\n",
        "    conv_out = Conv1DLayer(reformConvpool, 64, 3)\n",
        "    conv_out = FlattenLayer(conv_out)\n",
        "    # Input to LSTM should have the shape as (batch size, SEQ_LENGTH, num_features)\n",
        "    lstm = LSTMLayer(convpool, num_units=128, grad_clipping=grad_clip,\n",
        "        nonlinearity=lasagne.nonlinearities.tanh)\n",
        "    lstm_out = SliceLayer(lstm, -1, 1)\n",
        "    # Merge 1D-Conv and LSTM outputs\n",
        "    dense_input = ConcatLayer([conv_out, lstm_out])\n",
        "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
        "    convpool = DenseLayer(lasagne.layers.dropout(dense_input, p=.5),\n",
        "            num_units=512, nonlinearity=lasagne.nonlinearities.rectify)\n",
        "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
        "    convpool = DenseLayer(convpool,\n",
        "            num_units=nb_classes, nonlinearity=lasagne.nonlinearities.softmax)\n",
        "    return convpool\n",
        "\n",
        "\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    \"\"\"\n",
        "    Iterates over the samples returing batches of size batchsize.\n",
        "    :param inputs: input data array. It should be a 4D numpy array for images [n_samples, n_colors, W, H] and 5D numpy\n",
        "                    array if working with sequence of images [n_timewindows, n_samples, n_colors, W, H].\n",
        "    :param targets: vector of target labels.\n",
        "    :param batchsize: Batch size\n",
        "    :param shuffle: Flag whether to shuffle the samples before iterating or not.\n",
        "    :return: images and labels for a batch\n",
        "    \"\"\"\n",
        "    if inputs.ndim == 4:\n",
        "        input_len = inputs.shape[0]\n",
        "    elif inputs.ndim == 5:\n",
        "        input_len = inputs.shape[1]\n",
        "    assert input_len == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.arange(input_len)\n",
        "        np.random.shuffle(indices)\n",
        "    for start_idx in range(0, input_len, batchsize):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        if inputs.ndim == 4:\n",
        "            yield inputs[excerpt], targets[excerpt]\n",
        "        elif inputs.ndim == 5:\n",
        "            yield inputs[:, excerpt], targets[excerpt]\n",
        "\n",
        "\n",
        "def train(images, labels, fold, model_type, batch_size=32, num_epochs=5):\n",
        "    \"\"\"\n",
        "    A sample training function which loops over the training set and evaluates the network\n",
        "    on the validation set after each epoch. Evaluates the network on the training set\n",
        "    whenever the\n",
        "    :param images: input images\n",
        "    :param labels: target labels\n",
        "    :param fold: tuple of (train, test) index numbers\n",
        "    :param model_type: model type ('cnn', '1dconv', 'maxpool', 'lstm', 'mix')\n",
        "    :param batch_size: batch size for training\n",
        "    :param num_epochs: number of epochs of dataset to go over for training\n",
        "    :return: none\n",
        "    \"\"\"\n",
        "    num_classes = len(np.unique(labels))\n",
        "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = reformatInput(images, labels, fold)\n",
        "    X_train = X_train.astype(\"float32\", casting='unsafe')\n",
        "    X_val = X_val.astype(\"float32\", casting='unsafe')\n",
        "    X_test = X_test.astype(\"float32\", casting='unsafe')\n",
        "    # Prepare Theano variables for inputs and targets\n",
        "    input_var = T.TensorType('floatX', ((False,) * 5))()\n",
        "    target_var = T.ivector('targets')\n",
        "    # Create neural network model (depending on first command line parameter)\n",
        "    print(\"Building model and compiling functions...\")\n",
        "    # Building the appropriate model\n",
        "    if model_type == '1dconv':\n",
        "        network = build_convpool_conv1d(input_var, num_classes)\n",
        "    elif model_type == 'maxpool':\n",
        "        network = build_convpool_max(input_var, num_classes)\n",
        "    elif model_type == 'lstm':\n",
        "        network = build_convpool_lstm(input_var, num_classes, 100)\n",
        "    elif model_type == 'mix':\n",
        "        network = build_convpool_mix(input_var, num_classes, 100)\n",
        "    elif model_type == 'cnn':\n",
        "        input_var = T.tensor4('inputs')\n",
        "        network, _ = build_cnn(input_var)\n",
        "        network = DenseLayer(lasagne.layers.dropout(network, p=.5),\n",
        "                             num_units=256,\n",
        "                             nonlinearity=lasagne.nonlinearities.rectify)\n",
        "        network = DenseLayer(lasagne.layers.dropout(network, p=.5),\n",
        "                             num_units=num_classes,\n",
        "                             nonlinearity=lasagne.nonlinearities.softmax)\n",
        "    else:\n",
        "        raise ValueError(\"Model not supported ['1dconv', 'maxpool', 'lstm', 'mix', 'cnn']\")\n",
        "    # Create a loss expression for training, i.e., a scalar objective we want\n",
        "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
        "    prediction = lasagne.layers.get_output(network)\n",
        "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
        "    loss = loss.mean()\n",
        "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
        "    updates = lasagne.updates.adam(loss, params, learning_rate=0.001)\n",
        "    # Create a loss expression for validation/testing. The crucial difference\n",
        "    # here is that we do a deterministic forward pass through the network,\n",
        "    # disabling dropout layers.\n",
        "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
        "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
        "                                                            target_var)\n",
        "    test_loss = test_loss.mean()\n",
        "    # As a bonus, also create an expression for the classification accuracy:\n",
        "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
        "                      dtype=theano.config.floatX)\n",
        "    # Compile a function performing a training step on a mini-batch (by giving\n",
        "    # the updates dictionary) and returning the corresponding training loss:\n",
        "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
        "    # Compile a second function computing the validation loss and accuracy:\n",
        "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
        "    # Finally, launch the training loop.\n",
        "    print(\"Starting training...\")\n",
        "    best_validation_accu = 0\n",
        "    # We iterate over epochs:\n",
        "    for epoch in range(num_epochs):\n",
        "        # In each epoch, we do a full pass over the training data:\n",
        "        train_err = 0\n",
        "        train_batches = 0\n",
        "        start_time = time.time()\n",
        "        for batch in iterate_minibatches(X_train, y_train, batch_size, shuffle=False):\n",
        "            inputs, targets = batch\n",
        "            train_err += train_fn(inputs, targets)\n",
        "            train_batches += 1\n",
        "        # And a full pass over the validation data:\n",
        "        val_err = 0\n",
        "        val_acc = 0\n",
        "        val_batches = 0\n",
        "        for batch in iterate_minibatches(X_val, y_val, batch_size, shuffle=False):\n",
        "            inputs, targets = batch\n",
        "            err, acc = val_fn(inputs, targets)\n",
        "            val_err += err\n",
        "            val_acc += acc\n",
        "            val_batches += 1\n",
        "        av_train_err = train_err / train_batches\n",
        "        av_val_err = val_err / val_batches\n",
        "        av_val_acc = val_acc / val_batches\n",
        "        # Then we print the results for this epoch:\n",
        "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "            epoch + 1, num_epochs, time.time() - start_time))\n",
        "        print(\"  training loss:\\t\\t{:.6f}\".format(av_train_err))\n",
        "        print(\"  validation loss:\\t\\t{:.6f}\".format(av_val_err))\n",
        "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(av_val_acc * 100))\n",
        "        if av_val_acc > best_validation_accu:\n",
        "            best_validation_accu = av_val_acc\n",
        "            # After training, we compute and print the test error:\n",
        "            test_err = 0\n",
        "            test_acc = 0\n",
        "            test_batches = 0\n",
        "            for batch in iterate_minibatches(X_test, y_test, batch_size, shuffle=False):\n",
        "                inputs, targets = batch\n",
        "                err, acc = val_fn(inputs, targets)\n",
        "                test_err += err\n",
        "                test_acc += acc\n",
        "                test_batches += 1\n",
        "            av_test_err = test_err / test_batches\n",
        "            av_test_acc = test_acc / test_batches\n",
        "            print(\"Final results:\")\n",
        "            print(\"  test loss:\\t\\t\\t{:.6f}\".format(av_test_err))\n",
        "            print(\"  test accuracy:\\t\\t{:.2f} %\".format(av_test_acc * 100))\n",
        "            # Dump the network weights to a file like this:\n",
        "            np.savez('weights_lasg_{0}'.format(model_type), *lasagne.layers.get_all_param_values(network))\n",
        "    print('-'*50)\n",
        "    print(\"Best validation accuracy:\\t\\t{:.2f} %\".format(best_validation_accu * 100))\n",
        "    print(\"Best test accuracy:\\t\\t{:.2f} %\".format(av_test_acc * 100))\n",
        "\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    from utils import reformatInput\n",
        "    # Load electrode locations\n",
        "    print('Loading data...')\n",
        "    locs = scipy.io.loadmat('../Sample data/Neuroscan_locs_orig.mat')\n",
        "    locs_3d = locs['A']\n",
        "    locs_2d = []\n",
        "    # Convert to 2D\n",
        "    for e in locs_3d:\n",
        "        locs_2d.append(azim_proj(e))\n",
        "    feats = scipy.io.loadmat('../Sample data/FeatureMat_timeWin.mat')['features']\n",
        "    print ('Feats Shape: ',feats.shape)\n",
        "    subj_nums = np.squeeze(scipy.io.loadmat('../Sample data/trials_subNums.mat')['subjectNum'])\n",
        "    # Leave-Subject-Out cross validation\n",
        "    fold_pairs = []\n",
        "    for i in np.unique(subj_nums):\n",
        "        ts = subj_nums == i\n",
        "        tr = np.squeeze(np.nonzero(np.bitwise_not(ts)))\n",
        "        ts = np.squeeze(np.nonzero(ts))\n",
        "        np.random.shuffle(tr)  # Shuffle indices\n",
        "        np.random.shuffle(ts)\n",
        "        fold_pairs.append((tr, ts))\n",
        "    # CNN Mode\n",
        "    print('Generating images...')\n",
        "    # Find the average response over time windows\n",
        "    av_feats = reduce(lambda x, y: x+y, [feats[:, i*192:(i+1)*192] for i in range(feats.shape[1] / 192)])\n",
        "    av_feats = av_feats / (feats.shape[1] / 192)\n",
        "    images = gen_images(np.array(locs_2d),\n",
        "                                  av_feats,\n",
        "                                  32, normalize=False)\n",
        "    print('\\n')\n",
        "    # Class labels should start from 0\n",
        "    print('Training the CNN Model...')\n",
        "    train(images, np.squeeze(feats[:, -1]) - 1, fold_pairs[2], 'cnn')\n",
        "    # Conv-LSTM Mode\n",
        "    print('Generating images for all time windows...')\n",
        "    images_timewin = np.array([gen_images(np.array(locs_2d),\n",
        "                                                    feats[:, i * 192:(i + 1) * 192], 32, normalize=False) for i in\n",
        "                                         range(feats.shape[1] / 192)\n",
        "                                         ])\n",
        "    print('\\n')\n",
        "    print('Training the LSTM-CONV Model...')\n",
        "    train(images_timewin, np.squeeze(feats[:, -1]) - 1, fold_pairs[2], 'mix')\n",
        "    print('Done!')\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "re3l5zkhFjO6",
        "outputId": "f8e20d91-7d14-4ad9-a3d9-4edc8e99910f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nif __name__ == '__main__':\\n    from utils import reformatInput\\n    # Load electrode locations\\n    print('Loading data...')\\n    locs = scipy.io.loadmat('../Sample data/Neuroscan_locs_orig.mat')\\n    locs_3d = locs['A']\\n    locs_2d = []\\n    # Convert to 2D\\n    for e in locs_3d:\\n        locs_2d.append(azim_proj(e))\\n    feats = scipy.io.loadmat('../Sample data/FeatureMat_timeWin.mat')['features']\\n    print ('Feats Shape: ',feats.shape)\\n    subj_nums = np.squeeze(scipy.io.loadmat('../Sample data/trials_subNums.mat')['subjectNum'])\\n    # Leave-Subject-Out cross validation\\n    fold_pairs = []\\n    for i in np.unique(subj_nums):\\n        ts = subj_nums == i\\n        tr = np.squeeze(np.nonzero(np.bitwise_not(ts)))\\n        ts = np.squeeze(np.nonzero(ts))\\n        np.random.shuffle(tr)  # Shuffle indices\\n        np.random.shuffle(ts)\\n        fold_pairs.append((tr, ts))\\n    # CNN Mode\\n    print('Generating images...')\\n    # Find the average response over time windows\\n    av_feats = reduce(lambda x, y: x+y, [feats[:, i*192:(i+1)*192] for i in range(feats.shape[1] / 192)])\\n    av_feats = av_feats / (feats.shape[1] / 192)\\n    images = gen_images(np.array(locs_2d),\\n                                  av_feats,\\n                                  32, normalize=False)\\n    print('\\n')\\n    # Class labels should start from 0\\n    print('Training the CNN Model...')\\n    train(images, np.squeeze(feats[:, -1]) - 1, fold_pairs[2], 'cnn')\\n    # Conv-LSTM Mode\\n    print('Generating images for all time windows...')\\n    images_timewin = np.array([gen_images(np.array(locs_2d),\\n                                                    feats[:, i * 192:(i + 1) * 192], 32, normalize=False) for i in\\n                                         range(feats.shape[1] / 192)\\n                                         ])\\n    print('\\n')\\n    print('Training the LSTM-CONV Model...')\\n    train(images_timewin, np.squeeze(feats[:, -1]) - 1, fold_pairs[2], 'mix')\\n    print('Done!')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "print(keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cz7TfPlEwNb",
        "outputId": "59e70104-ace2-4951-98e3-b7515299f64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NZFMq40fEuq0",
        "outputId": "3d542e98-b37b-45fb-d8f0-f43f991de8d9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as scs\n",
        "import re\n",
        "from numpy import genfromtxt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "plt.rcParams[\"figure.figsize\"] = (12,12)\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.precision = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAdtd6uZEuq3"
      },
      "source": [
        "## Brainwave Frequencies:\n",
        "Gamma, 30 to 50 Hz.  \n",
        "Beta, 14 to 30 Hz.  \n",
        "Alpha, 8 to 14 Hz.  \n",
        "Theta, 4 to 8 Hz.  \n",
        "Delta, 0.1 to 4 Hz.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDeOFVApEuq4"
      },
      "source": [
        "## Changing Bin Size: \n",
        "https://stackoverflow.com/questions/25735153/plotting-a-fast-fourier-transform-in-python  \n",
        "(Search for 'bin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20HY6vMFEuq5"
      },
      "source": [
        "An EEG processing library:  \n",
        "https://github.com/pbashivan/EEGLearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "zxhuzKX0Euq5"
      },
      "outputs": [],
      "source": [
        "theta = (4,8)\n",
        "alpha = (8,12)\n",
        "beta = (12,40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z1yjwWnhEuq6"
      },
      "outputs": [],
      "source": [
        "def get_fft(snippet):\n",
        "    Fs = 128.0;  # sampling rate\n",
        "    #Ts = len(snippet)/Fs/Fs; # sampling interval\n",
        "    snippet_time = len(snippet)/Fs\n",
        "    Ts = 1.0/Fs; # sampling interval\n",
        "    t = np.arange(0,snippet_time,Ts) # time vector\n",
        "\n",
        "    # ff = 5;   # frequency of the signal\n",
        "    # y = np.sin(2*np.pi*ff*t)\n",
        "    y = snippet\n",
        "#     print('Ts: ',Ts)\n",
        "#     print(t)\n",
        "#     print(y.shape)\n",
        "    n = len(y) # length of the signal\n",
        "    k = np.arange(n)\n",
        "    T = n/Fs\n",
        "    frq = k/T # two sides frequency range\n",
        "    frq = frq[range(n//2)] # one side frequency range\n",
        "\n",
        "    Y = np.fft.fft(y)/n # fft computing and normalization\n",
        "    Y = Y[range(n//2)]\n",
        "    #Added in: (To remove bias.)\n",
        "    #Y[0] = 0\n",
        "    return frq,abs(Y)\n",
        "#f,Y = get_fft(np.hanning(len(snippet))*snippet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "MvqQH_ywEuq7"
      },
      "outputs": [],
      "source": [
        "def theta_alpha_beta_averages(f,Y):\n",
        "    theta_range = (4,8)\n",
        "    alpha_range = (8,12)\n",
        "    beta_range = (12,40)\n",
        "    theta = Y[(f>theta_range[0]) & (f<=theta_range[1])].mean()\n",
        "    alpha = Y[(f>alpha_range[0]) & (f<=alpha_range[1])].mean()\n",
        "    beta = Y[(f>beta_range[0]) & (f<=beta_range[1])].mean()\n",
        "    return theta, alpha, beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EzvavUheEuq8"
      },
      "outputs": [],
      "source": [
        "def make_steps(samples,frame_duration,overlap):\n",
        "    '''\n",
        "    in:\n",
        "    samples - number of samples in the session\n",
        "    frame_duration - frame duration in seconds \n",
        "    overlap - float fraction of frame to overlap in range (0,1)\n",
        "    \n",
        "    out: list of tuple ranges\n",
        "    '''\n",
        "    #steps = np.arange(0,len(df),frame_length)\n",
        "    Fs = 128\n",
        "    i = 0\n",
        "    intervals = []\n",
        "    samples_per_frame = Fs * frame_duration\n",
        "    while i+samples_per_frame <= samples:\n",
        "        intervals.append((i,i+samples_per_frame))\n",
        "        i = i + samples_per_frame - int(samples_per_frame*overlap)\n",
        "    return intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dMcCUv0CEuq9"
      },
      "outputs": [],
      "source": [
        "def make_frames(df,frame_duration):\n",
        "    '''\n",
        "    in: dataframe or array with all channels, frame duration in seconds\n",
        "    out: array of theta, alpha, beta averages for each probe for each time step\n",
        "        shape: (n-frames,m-probes,k-brainwave bands)\n",
        "    '''\n",
        "    Fs = 128.0\n",
        "    frame_length = Fs*frame_duration\n",
        "    frames = []\n",
        "    steps = make_steps(len(df),frame_duration,overlap)\n",
        "    for i,_ in enumerate(steps):\n",
        "        frame = []\n",
        "        if i == 0:\n",
        "            continue\n",
        "        else:\n",
        "            for channel in df.columns:\n",
        "                snippet = np.array(df.loc[steps[i][0]:steps[i][1],int(channel)])\n",
        "                f,Y =  get_fft(snippet)\n",
        "                theta, alpha, beta = theta_alpha_beta_averages(f,Y)\n",
        "                frame.append([theta, alpha, beta])\n",
        "            \n",
        "        frames.append(frame)\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "v8kC6cXpEuq-"
      },
      "outputs": [],
      "source": [
        "locs_2d = [(-2.0,4.0),\n",
        "           (2.0,4.0),\n",
        "           (-1.0,3.0),\n",
        "           (1.0,3.0),\n",
        "           (-3.0,3.0),\n",
        "           (3.0,3.0),\n",
        "           (-2.0,2.0),\n",
        "           (2.0,2.0),\n",
        "           (-2.0,-2.0),\n",
        "           (2.0,-2.0),\n",
        "           (-4.0,1.0),\n",
        "           (4.0,1.0),\n",
        "           (-1.0,-3.0),\n",
        "           (1.0,-3.0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vUU_lr5mEuq_"
      },
      "outputs": [],
      "source": [
        "def make_data_pipeline(file_names,labels,image_size,frame_duration,overlap):\n",
        "    '''\n",
        "    IN: \n",
        "    file_names - list of strings for each input file (one for each subject)\n",
        "    labels - list of labels for each\n",
        "    image_size - int size of output images in form (x, x)\n",
        "    frame_duration - time length of each frame (seconds)\n",
        "    overlap - float fraction of frame to overlap in range (0,1)\n",
        "    \n",
        "    OUT:\n",
        "    X: np array of frames (unshuffled)\n",
        "    y: np array of label for each frame (1 or 0)\n",
        "    '''\n",
        "    ##################################\n",
        "    ###Still need to do the overlap###!!!\n",
        "    ##################################\n",
        "    \n",
        "    Fs = 128.0   #sampling rate\n",
        "    frame_length = Fs * frame_duration\n",
        "    \n",
        "    print('Generating training data...')\n",
        "    \n",
        "    \n",
        "    for i, file in enumerate(file_names):\n",
        "        print ('Processing session: ',file, '. (',i+1,' of ',len(file_names),')')\n",
        "        data = genfromtxt(file, delimiter=',').T\n",
        "        df = pd.DataFrame(data)\n",
        "        \n",
        "        X_0 = make_frames(df,frame_duration)\n",
        "        #steps = np.arange(0,len(df),frame_length)\n",
        "        X_1 = X_0.reshape(len(X_0),14*3)\n",
        "        \n",
        "        images = gen_images(np.array(locs_2d),X_1, image_size, normalize=False)\n",
        "        images = np.swapaxes(images, 1, 3) \n",
        "        print(len(images), ' frames generated with label ', labels[i], '.')\n",
        "        print('\\n')\n",
        "        if i == 0:\n",
        "            X = images\n",
        "            y = np.ones(len(images))*labels[0]\n",
        "        else:\n",
        "            X = np.concatenate((X,images),axis = 0)\n",
        "            y = np.concatenate((y,np.ones(len(images))*labels[i]),axis = 0)\n",
        "        \n",
        "        \n",
        "    return X,np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SISQGVVaEurA",
        "outputId": "14a3cf35-b263-4d23-e32a-482e940d505e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating training data...\n",
            "Processing session:  drive/MyDrive/newdata/ML101_KS.csv . ( 1  of  16 )\n",
            "234  frames generated with label  1 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML101_US.csv . ( 2  of  16 )\n",
            "224  frames generated with label  0 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML102_KS.csv . ( 3  of  16 )\n",
            "222  frames generated with label  1 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML102_US.csv . ( 4  of  16 )\n",
            "218  frames generated with label  0 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML103_KS.csv . ( 5  of  16 )\n",
            "226  frames generated with label  1 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML103_US.csv . ( 6  of  16 )\n",
            "208  frames generated with label  0 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML104_KS.csv . ( 7  of  16 )\n",
            "202  frames generated with label  1 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML104_US.csv . ( 8  of  16 )\n",
            "204  frames generated with label  0 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML105_KS.csv . ( 9  of  16 )\n",
            "214  frames generated with label  1 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML105_US.csv . ( 10  of  16 )\n",
            "226  frames generated with label  0 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML106_KS.csv . ( 11  of  16 )\n",
            "230  frames generated with label  1 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML106_US.csv . ( 12  of  16 )\n",
            "278  frames generated with label  0 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML107_KS.csv . ( 13  of  16 )\n",
            "246  frames generated with label  1 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML107_US.csv . ( 14  of  16 )\n",
            "236  frames generated with label  0 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML108_KS.csv . ( 15  of  16 )\n",
            "240  frames generated with label  1 .\n",
            "\n",
            "\n",
            "Processing session:  drive/MyDrive/newdata/ML108_US.csv . ( 16  of  16 )\n",
            "234  frames generated with label  0 .\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_names = ['drive/MyDrive/newdata/ML101_KS.csv',\n",
        "              'drive/MyDrive/newdata/ML101_US.csv',\n",
        "              'drive/MyDrive/newdata/ML102_KS.csv',\n",
        "              'drive/MyDrive/newdata/ML102_US.csv',\n",
        "              'drive/MyDrive/newdata/ML103_KS.csv',\n",
        "              'drive/MyDrive/newdata/ML103_US.csv',\n",
        "              'drive/MyDrive/newdata/ML104_KS.csv',\n",
        "              'drive/MyDrive/newdata/ML104_US.csv',\n",
        "              'drive/MyDrive/newdata/ML105_KS.csv',\n",
        "              'drive/MyDrive/newdata/ML105_US.csv',\n",
        "              'drive/MyDrive/newdata/ML106_KS.csv',\n",
        "              'drive/MyDrive/newdata/ML106_US.csv',\n",
        "              'drive/MyDrive/newdata/ML107_KS.csv',\n",
        "              'drive/MyDrive/newdata/ML107_US.csv',\n",
        "              'drive/MyDrive/newdata/ML108_KS.csv',\n",
        "              'drive/MyDrive/newdata/ML108_US.csv']\n",
        "labels = [1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0]\n",
        "image_size = 28\n",
        "frame_duration = 1.0\n",
        "overlap = 0.5\n",
        "X, y = make_data_pipeline(file_names,labels,image_size,frame_duration,overlap)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axH0ORoEE-YA",
        "outputId": "5c2ac8dd-6f75-4284-bb97-c94c0aeb0781"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmGEpo-REurA",
        "outputId": "ef6d920e-a2d9-44b9-b6f3-a8b496a7e6cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3642, 28, 28, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5Tma4qwEurB",
        "outputId": "b7604451-c5bb-4526-e956-8d1bafdc1026"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3642,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "5OKFmICcEurC",
        "outputId": "9aaa16da-77df-4bfb-95c0-dcf14562069f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd2158eded0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "X5zERNq5EurC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jlW6o0zEurD",
        "outputId": "76295646-970a-4a45-96a6-48f9a379044e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2913,)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIkz-hN71Bp6",
        "outputId": "6f6250b4-1ec8-4226-c348-70e2749e86d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2913, 28, 28, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXdJXkjF1W-L",
        "outputId": "5174a0b1-174c-4c70-a487-1e27dc6e7e37"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(729, 28, 28, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze3W4AFo1lCL",
        "outputId": "dacb24dc-5917-436e-d7db-9eac2569a495"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(729,)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHwkTWtVEurD",
        "outputId": "5af1331c-0ebe-44ec-c822-38c8570f26b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (2913, 28, 28, 3)\n",
            "2913 train samples\n",
            "(729, 28, 28, 3) test samples\n"
          ]
        }
      ],
      "source": [
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train /= 255\n",
        "#x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape, 'test samples')\n",
        "\n",
        "input_shape = (img_rows, img_cols, 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install q keras==2.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXVIk-XPMFWS",
        "outputId": "b06a31f9-1445-4120-e295-4581f901d7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: q in /usr/local/lib/python3.7/dist-packages (2.6)\n",
            "Requirement already satisfied: keras==2.8.0 in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGTIq-RLEurE",
        "outputId": "02d89b61-3a55-4a57-a4d6-4a014edd833f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "92/92 [==============================] - 8s 73ms/step - loss: 0.6953 - accuracy: 0.5005 - val_loss: 0.6912 - val_accuracy: 0.5158\n",
            "Epoch 2/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.6853 - accuracy: 0.5307 - val_loss: 0.7138 - val_accuracy: 0.5226\n",
            "Epoch 3/200\n",
            "92/92 [==============================] - 6s 71ms/step - loss: 0.6771 - accuracy: 0.5424 - val_loss: 0.6870 - val_accuracy: 0.5158\n",
            "Epoch 4/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.6648 - accuracy: 0.5716 - val_loss: 0.6638 - val_accuracy: 0.5967\n",
            "Epoch 5/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.6605 - accuracy: 0.5826 - val_loss: 0.6599 - val_accuracy: 0.6049\n",
            "Epoch 6/200\n",
            "92/92 [==============================] - 6s 71ms/step - loss: 0.6548 - accuracy: 0.5915 - val_loss: 0.6617 - val_accuracy: 0.5953\n",
            "Epoch 7/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.6480 - accuracy: 0.6025 - val_loss: 0.6563 - val_accuracy: 0.5816\n",
            "Epoch 8/200\n",
            "92/92 [==============================] - 6s 71ms/step - loss: 0.6432 - accuracy: 0.6121 - val_loss: 0.6612 - val_accuracy: 0.5967\n",
            "Epoch 9/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.6392 - accuracy: 0.6227 - val_loss: 0.6459 - val_accuracy: 0.6022\n",
            "Epoch 10/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.6336 - accuracy: 0.6354 - val_loss: 0.6478 - val_accuracy: 0.6118\n",
            "Epoch 11/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.6272 - accuracy: 0.6272 - val_loss: 0.6436 - val_accuracy: 0.6132\n",
            "Epoch 12/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.6237 - accuracy: 0.6402 - val_loss: 0.6351 - val_accuracy: 0.6351\n",
            "Epoch 13/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.6223 - accuracy: 0.6413 - val_loss: 0.6257 - val_accuracy: 0.6337\n",
            "Epoch 14/200\n",
            "92/92 [==============================] - 7s 81ms/step - loss: 0.6144 - accuracy: 0.6495 - val_loss: 0.6226 - val_accuracy: 0.6269\n",
            "Epoch 15/200\n",
            "92/92 [==============================] - 12s 133ms/step - loss: 0.6149 - accuracy: 0.6488 - val_loss: 0.7206 - val_accuracy: 0.5569\n",
            "Epoch 16/200\n",
            "92/92 [==============================] - 9s 94ms/step - loss: 0.6100 - accuracy: 0.6533 - val_loss: 0.6554 - val_accuracy: 0.6214\n",
            "Epoch 17/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.6014 - accuracy: 0.6584 - val_loss: 0.6156 - val_accuracy: 0.6653\n",
            "Epoch 18/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.6013 - accuracy: 0.6670 - val_loss: 0.6952 - val_accuracy: 0.6145\n",
            "Epoch 19/200\n",
            "92/92 [==============================] - 8s 92ms/step - loss: 0.5989 - accuracy: 0.6735 - val_loss: 0.6746 - val_accuracy: 0.5967\n",
            "Epoch 20/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.5926 - accuracy: 0.6804 - val_loss: 0.6222 - val_accuracy: 0.6187\n",
            "Epoch 21/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.5880 - accuracy: 0.6910 - val_loss: 0.6216 - val_accuracy: 0.6324\n",
            "Epoch 22/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.5852 - accuracy: 0.6838 - val_loss: 0.6505 - val_accuracy: 0.6049\n",
            "Epoch 23/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5837 - accuracy: 0.6859 - val_loss: 0.6156 - val_accuracy: 0.6241\n",
            "Epoch 24/200\n",
            "92/92 [==============================] - 6s 71ms/step - loss: 0.5772 - accuracy: 0.6931 - val_loss: 0.6663 - val_accuracy: 0.5981\n",
            "Epoch 25/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5782 - accuracy: 0.6835 - val_loss: 0.6755 - val_accuracy: 0.5898\n",
            "Epoch 26/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5689 - accuracy: 0.6993 - val_loss: 0.6432 - val_accuracy: 0.6173\n",
            "Epoch 27/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.5710 - accuracy: 0.7044 - val_loss: 0.6303 - val_accuracy: 0.6310\n",
            "Epoch 28/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.5668 - accuracy: 0.7020 - val_loss: 0.6236 - val_accuracy: 0.6708\n",
            "Epoch 29/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5617 - accuracy: 0.7051 - val_loss: 0.6078 - val_accuracy: 0.6379\n",
            "Epoch 30/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.5588 - accuracy: 0.7130 - val_loss: 0.6471 - val_accuracy: 0.6187\n",
            "Epoch 31/200\n",
            "92/92 [==============================] - 8s 82ms/step - loss: 0.5611 - accuracy: 0.7048 - val_loss: 0.5901 - val_accuracy: 0.6804\n",
            "Epoch 32/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5515 - accuracy: 0.7144 - val_loss: 0.6042 - val_accuracy: 0.6379\n",
            "Epoch 33/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.5513 - accuracy: 0.7178 - val_loss: 0.6137 - val_accuracy: 0.6543\n",
            "Epoch 34/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5475 - accuracy: 0.7230 - val_loss: 0.5861 - val_accuracy: 0.6763\n",
            "Epoch 35/200\n",
            "92/92 [==============================] - 6s 70ms/step - loss: 0.5448 - accuracy: 0.7230 - val_loss: 0.5858 - val_accuracy: 0.6749\n",
            "Epoch 36/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5405 - accuracy: 0.7247 - val_loss: 0.7004 - val_accuracy: 0.6049\n",
            "Epoch 37/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5406 - accuracy: 0.7302 - val_loss: 0.5919 - val_accuracy: 0.6941\n",
            "Epoch 38/200\n",
            "92/92 [==============================] - 6s 71ms/step - loss: 0.5368 - accuracy: 0.7254 - val_loss: 0.6033 - val_accuracy: 0.6900\n",
            "Epoch 39/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5340 - accuracy: 0.7305 - val_loss: 0.6153 - val_accuracy: 0.6886\n",
            "Epoch 40/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5340 - accuracy: 0.7418 - val_loss: 0.5907 - val_accuracy: 0.6612\n",
            "Epoch 41/200\n",
            "92/92 [==============================] - 6s 71ms/step - loss: 0.5284 - accuracy: 0.7305 - val_loss: 0.6380 - val_accuracy: 0.6708\n",
            "Epoch 42/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5287 - accuracy: 0.7343 - val_loss: 0.6135 - val_accuracy: 0.6749\n",
            "Epoch 43/200\n",
            "92/92 [==============================] - 6s 71ms/step - loss: 0.5229 - accuracy: 0.7346 - val_loss: 0.6064 - val_accuracy: 0.6557\n",
            "Epoch 44/200\n",
            "92/92 [==============================] - 6s 71ms/step - loss: 0.5239 - accuracy: 0.7370 - val_loss: 0.7651 - val_accuracy: 0.5802\n",
            "Epoch 45/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5234 - accuracy: 0.7384 - val_loss: 0.6273 - val_accuracy: 0.6516\n",
            "Epoch 46/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5169 - accuracy: 0.7405 - val_loss: 0.6138 - val_accuracy: 0.6557\n",
            "Epoch 47/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5162 - accuracy: 0.7494 - val_loss: 0.5829 - val_accuracy: 0.6831\n",
            "Epoch 48/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5145 - accuracy: 0.7460 - val_loss: 0.5886 - val_accuracy: 0.6749\n",
            "Epoch 49/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5151 - accuracy: 0.7446 - val_loss: 0.6038 - val_accuracy: 0.6667\n",
            "Epoch 50/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.5114 - accuracy: 0.7552 - val_loss: 0.6701 - val_accuracy: 0.6187\n",
            "Epoch 51/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.5077 - accuracy: 0.7501 - val_loss: 0.6403 - val_accuracy: 0.6502\n",
            "Epoch 52/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.5090 - accuracy: 0.7563 - val_loss: 0.5851 - val_accuracy: 0.6831\n",
            "Epoch 53/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.5015 - accuracy: 0.7532 - val_loss: 0.6490 - val_accuracy: 0.6475\n",
            "Epoch 54/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.5013 - accuracy: 0.7549 - val_loss: 0.6723 - val_accuracy: 0.6543\n",
            "Epoch 55/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.5021 - accuracy: 0.7600 - val_loss: 0.5675 - val_accuracy: 0.7064\n",
            "Epoch 56/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4997 - accuracy: 0.7576 - val_loss: 0.7290 - val_accuracy: 0.6118\n",
            "Epoch 57/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.5008 - accuracy: 0.7477 - val_loss: 0.6147 - val_accuracy: 0.6598\n",
            "Epoch 58/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4943 - accuracy: 0.7590 - val_loss: 0.5830 - val_accuracy: 0.7010\n",
            "Epoch 59/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4947 - accuracy: 0.7600 - val_loss: 0.6056 - val_accuracy: 0.6612\n",
            "Epoch 60/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4914 - accuracy: 0.7673 - val_loss: 0.6023 - val_accuracy: 0.6872\n",
            "Epoch 61/200\n",
            "92/92 [==============================] - 7s 81ms/step - loss: 0.4843 - accuracy: 0.7587 - val_loss: 0.6183 - val_accuracy: 0.6639\n",
            "Epoch 62/200\n",
            "92/92 [==============================] - 8s 82ms/step - loss: 0.4872 - accuracy: 0.7686 - val_loss: 0.5791 - val_accuracy: 0.7023\n",
            "Epoch 63/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4829 - accuracy: 0.7669 - val_loss: 0.6123 - val_accuracy: 0.6982\n",
            "Epoch 64/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4826 - accuracy: 0.7635 - val_loss: 0.5798 - val_accuracy: 0.7119\n",
            "Epoch 65/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4824 - accuracy: 0.7624 - val_loss: 0.6123 - val_accuracy: 0.6667\n",
            "Epoch 66/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4770 - accuracy: 0.7621 - val_loss: 0.6872 - val_accuracy: 0.6433\n",
            "Epoch 67/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4868 - accuracy: 0.7648 - val_loss: 0.6010 - val_accuracy: 0.6845\n",
            "Epoch 68/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4760 - accuracy: 0.7642 - val_loss: 0.6151 - val_accuracy: 0.6804\n",
            "Epoch 69/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4757 - accuracy: 0.7693 - val_loss: 0.6202 - val_accuracy: 0.6680\n",
            "Epoch 70/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4754 - accuracy: 0.7652 - val_loss: 0.5956 - val_accuracy: 0.6831\n",
            "Epoch 71/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4723 - accuracy: 0.7727 - val_loss: 0.5623 - val_accuracy: 0.7160\n",
            "Epoch 72/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4679 - accuracy: 0.7772 - val_loss: 0.5953 - val_accuracy: 0.6845\n",
            "Epoch 73/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4617 - accuracy: 0.7748 - val_loss: 0.5746 - val_accuracy: 0.7257\n",
            "Epoch 74/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4659 - accuracy: 0.7769 - val_loss: 0.6438 - val_accuracy: 0.6680\n",
            "Epoch 75/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4630 - accuracy: 0.7731 - val_loss: 0.5723 - val_accuracy: 0.7160\n",
            "Epoch 76/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4657 - accuracy: 0.7659 - val_loss: 0.5729 - val_accuracy: 0.7023\n",
            "Epoch 77/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4597 - accuracy: 0.7834 - val_loss: 0.5605 - val_accuracy: 0.7257\n",
            "Epoch 78/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4573 - accuracy: 0.7806 - val_loss: 0.6037 - val_accuracy: 0.6818\n",
            "Epoch 79/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4593 - accuracy: 0.7782 - val_loss: 0.6386 - val_accuracy: 0.6763\n",
            "Epoch 80/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4607 - accuracy: 0.7734 - val_loss: 0.6035 - val_accuracy: 0.6914\n",
            "Epoch 81/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4505 - accuracy: 0.7875 - val_loss: 0.6684 - val_accuracy: 0.6996\n",
            "Epoch 82/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4517 - accuracy: 0.7841 - val_loss: 0.5881 - val_accuracy: 0.6859\n",
            "Epoch 83/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4515 - accuracy: 0.7830 - val_loss: 0.5876 - val_accuracy: 0.6941\n",
            "Epoch 84/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4501 - accuracy: 0.7872 - val_loss: 0.5596 - val_accuracy: 0.7229\n",
            "Epoch 85/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4476 - accuracy: 0.7889 - val_loss: 0.6419 - val_accuracy: 0.6722\n",
            "Epoch 86/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4475 - accuracy: 0.7851 - val_loss: 0.5965 - val_accuracy: 0.6968\n",
            "Epoch 87/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4458 - accuracy: 0.7947 - val_loss: 0.5794 - val_accuracy: 0.7010\n",
            "Epoch 88/200\n",
            "92/92 [==============================] - 7s 71ms/step - loss: 0.4439 - accuracy: 0.7878 - val_loss: 0.5841 - val_accuracy: 0.6900\n",
            "Epoch 89/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4441 - accuracy: 0.7878 - val_loss: 0.5868 - val_accuracy: 0.6900\n",
            "Epoch 90/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4372 - accuracy: 0.7964 - val_loss: 0.5890 - val_accuracy: 0.7106\n",
            "Epoch 91/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4407 - accuracy: 0.7916 - val_loss: 0.6190 - val_accuracy: 0.6900\n",
            "Epoch 92/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.4404 - accuracy: 0.7933 - val_loss: 0.6031 - val_accuracy: 0.6818\n",
            "Epoch 93/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4337 - accuracy: 0.8002 - val_loss: 0.5496 - val_accuracy: 0.7284\n",
            "Epoch 94/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4343 - accuracy: 0.7964 - val_loss: 0.5505 - val_accuracy: 0.7202\n",
            "Epoch 95/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4308 - accuracy: 0.7985 - val_loss: 0.5905 - val_accuracy: 0.7010\n",
            "Epoch 96/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.4309 - accuracy: 0.7964 - val_loss: 0.6288 - val_accuracy: 0.7023\n",
            "Epoch 97/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.4299 - accuracy: 0.7940 - val_loss: 0.5746 - val_accuracy: 0.7243\n",
            "Epoch 98/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4327 - accuracy: 0.7916 - val_loss: 0.5854 - val_accuracy: 0.7010\n",
            "Epoch 99/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4230 - accuracy: 0.8033 - val_loss: 0.5698 - val_accuracy: 0.7174\n",
            "Epoch 100/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.4242 - accuracy: 0.8023 - val_loss: 0.5500 - val_accuracy: 0.7270\n",
            "Epoch 101/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.4240 - accuracy: 0.8081 - val_loss: 0.6514 - val_accuracy: 0.6831\n",
            "Epoch 102/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.4248 - accuracy: 0.8060 - val_loss: 0.6126 - val_accuracy: 0.6968\n",
            "Epoch 103/200\n",
            "92/92 [==============================] - 7s 75ms/step - loss: 0.4230 - accuracy: 0.8026 - val_loss: 0.5498 - val_accuracy: 0.7270\n",
            "Epoch 104/200\n",
            "92/92 [==============================] - 9s 94ms/step - loss: 0.4205 - accuracy: 0.8081 - val_loss: 0.6188 - val_accuracy: 0.6694\n",
            "Epoch 105/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4223 - accuracy: 0.8057 - val_loss: 0.5780 - val_accuracy: 0.7133\n",
            "Epoch 106/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4209 - accuracy: 0.8091 - val_loss: 0.5451 - val_accuracy: 0.7284\n",
            "Epoch 107/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4109 - accuracy: 0.8098 - val_loss: 0.5881 - val_accuracy: 0.7284\n",
            "Epoch 108/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4186 - accuracy: 0.8057 - val_loss: 0.6230 - val_accuracy: 0.6900\n",
            "Epoch 109/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4154 - accuracy: 0.8112 - val_loss: 0.5382 - val_accuracy: 0.7298\n",
            "Epoch 110/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4121 - accuracy: 0.8043 - val_loss: 0.6229 - val_accuracy: 0.6886\n",
            "Epoch 111/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4109 - accuracy: 0.8098 - val_loss: 0.5533 - val_accuracy: 0.7380\n",
            "Epoch 112/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4087 - accuracy: 0.8064 - val_loss: 0.6315 - val_accuracy: 0.6872\n",
            "Epoch 113/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4050 - accuracy: 0.8160 - val_loss: 0.5953 - val_accuracy: 0.7202\n",
            "Epoch 114/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4066 - accuracy: 0.8136 - val_loss: 0.5435 - val_accuracy: 0.7202\n",
            "Epoch 115/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4025 - accuracy: 0.8136 - val_loss: 0.6425 - val_accuracy: 0.6859\n",
            "Epoch 116/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.4083 - accuracy: 0.8163 - val_loss: 0.5630 - val_accuracy: 0.7325\n",
            "Epoch 117/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4073 - accuracy: 0.8153 - val_loss: 0.6323 - val_accuracy: 0.6914\n",
            "Epoch 118/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3993 - accuracy: 0.8146 - val_loss: 0.5459 - val_accuracy: 0.7270\n",
            "Epoch 119/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3971 - accuracy: 0.8170 - val_loss: 0.6213 - val_accuracy: 0.7064\n",
            "Epoch 120/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4047 - accuracy: 0.8102 - val_loss: 0.6424 - val_accuracy: 0.7010\n",
            "Epoch 121/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.4001 - accuracy: 0.8136 - val_loss: 0.5461 - val_accuracy: 0.7353\n",
            "Epoch 122/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3988 - accuracy: 0.8222 - val_loss: 0.7186 - val_accuracy: 0.6818\n",
            "Epoch 123/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3958 - accuracy: 0.8170 - val_loss: 0.5286 - val_accuracy: 0.7407\n",
            "Epoch 124/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3955 - accuracy: 0.8266 - val_loss: 0.7518 - val_accuracy: 0.6598\n",
            "Epoch 125/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3968 - accuracy: 0.8242 - val_loss: 0.5663 - val_accuracy: 0.7215\n",
            "Epoch 126/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3938 - accuracy: 0.8143 - val_loss: 0.5400 - val_accuracy: 0.7503\n",
            "Epoch 127/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.3965 - accuracy: 0.8304 - val_loss: 0.5714 - val_accuracy: 0.7064\n",
            "Epoch 128/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3861 - accuracy: 0.8284 - val_loss: 0.6134 - val_accuracy: 0.7010\n",
            "Epoch 129/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3909 - accuracy: 0.8218 - val_loss: 0.6387 - val_accuracy: 0.6927\n",
            "Epoch 130/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.3846 - accuracy: 0.8266 - val_loss: 0.5519 - val_accuracy: 0.7380\n",
            "Epoch 131/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3885 - accuracy: 0.8273 - val_loss: 0.6706 - val_accuracy: 0.7092\n",
            "Epoch 132/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3924 - accuracy: 0.8263 - val_loss: 0.5555 - val_accuracy: 0.7325\n",
            "Epoch 133/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3870 - accuracy: 0.8318 - val_loss: 0.5886 - val_accuracy: 0.7078\n",
            "Epoch 134/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3809 - accuracy: 0.8297 - val_loss: 0.6672 - val_accuracy: 0.7174\n",
            "Epoch 135/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3849 - accuracy: 0.8256 - val_loss: 0.5390 - val_accuracy: 0.7284\n",
            "Epoch 136/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3780 - accuracy: 0.8297 - val_loss: 0.5388 - val_accuracy: 0.7476\n",
            "Epoch 137/200\n",
            "92/92 [==============================] - 7s 75ms/step - loss: 0.3803 - accuracy: 0.8318 - val_loss: 0.5326 - val_accuracy: 0.7380\n",
            "Epoch 138/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3742 - accuracy: 0.8290 - val_loss: 0.6189 - val_accuracy: 0.7119\n",
            "Epoch 139/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3750 - accuracy: 0.8380 - val_loss: 0.5248 - val_accuracy: 0.7613\n",
            "Epoch 140/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3792 - accuracy: 0.8297 - val_loss: 0.5400 - val_accuracy: 0.7503\n",
            "Epoch 141/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3765 - accuracy: 0.8335 - val_loss: 0.5431 - val_accuracy: 0.7202\n",
            "Epoch 142/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3740 - accuracy: 0.8349 - val_loss: 0.5635 - val_accuracy: 0.7366\n",
            "Epoch 143/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3715 - accuracy: 0.8359 - val_loss: 0.5793 - val_accuracy: 0.7078\n",
            "Epoch 144/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3707 - accuracy: 0.8290 - val_loss: 0.6117 - val_accuracy: 0.6982\n",
            "Epoch 145/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3696 - accuracy: 0.8359 - val_loss: 0.5909 - val_accuracy: 0.7133\n",
            "Epoch 146/200\n",
            "92/92 [==============================] - 8s 88ms/step - loss: 0.3719 - accuracy: 0.8363 - val_loss: 0.5330 - val_accuracy: 0.7394\n",
            "Epoch 147/200\n",
            "92/92 [==============================] - 7s 81ms/step - loss: 0.3647 - accuracy: 0.8369 - val_loss: 0.5376 - val_accuracy: 0.7325\n",
            "Epoch 148/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3668 - accuracy: 0.8363 - val_loss: 0.5390 - val_accuracy: 0.7298\n",
            "Epoch 149/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3659 - accuracy: 0.8459 - val_loss: 0.7006 - val_accuracy: 0.6804\n",
            "Epoch 150/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3644 - accuracy: 0.8411 - val_loss: 0.7211 - val_accuracy: 0.6914\n",
            "Epoch 151/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3665 - accuracy: 0.8397 - val_loss: 0.6247 - val_accuracy: 0.6982\n",
            "Epoch 152/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3668 - accuracy: 0.8383 - val_loss: 0.5248 - val_accuracy: 0.7517\n",
            "Epoch 153/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3589 - accuracy: 0.8462 - val_loss: 0.5231 - val_accuracy: 0.7449\n",
            "Epoch 154/200\n",
            "92/92 [==============================] - 7s 72ms/step - loss: 0.3579 - accuracy: 0.8435 - val_loss: 0.5888 - val_accuracy: 0.7257\n",
            "Epoch 155/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3612 - accuracy: 0.8438 - val_loss: 0.5555 - val_accuracy: 0.7284\n",
            "Epoch 156/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3591 - accuracy: 0.8441 - val_loss: 0.5796 - val_accuracy: 0.7215\n",
            "Epoch 157/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3541 - accuracy: 0.8424 - val_loss: 0.7507 - val_accuracy: 0.6694\n",
            "Epoch 158/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3581 - accuracy: 0.8404 - val_loss: 0.5401 - val_accuracy: 0.7517\n",
            "Epoch 159/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3524 - accuracy: 0.8465 - val_loss: 0.5220 - val_accuracy: 0.7517\n",
            "Epoch 160/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3509 - accuracy: 0.8486 - val_loss: 0.6254 - val_accuracy: 0.6968\n",
            "Epoch 161/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3511 - accuracy: 0.8441 - val_loss: 0.6307 - val_accuracy: 0.7147\n",
            "Epoch 162/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3504 - accuracy: 0.8548 - val_loss: 0.6888 - val_accuracy: 0.6927\n",
            "Epoch 163/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3501 - accuracy: 0.8455 - val_loss: 0.6105 - val_accuracy: 0.7037\n",
            "Epoch 164/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3517 - accuracy: 0.8380 - val_loss: 0.6174 - val_accuracy: 0.7092\n",
            "Epoch 165/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3464 - accuracy: 0.8472 - val_loss: 0.6575 - val_accuracy: 0.6996\n",
            "Epoch 166/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3435 - accuracy: 0.8469 - val_loss: 0.5572 - val_accuracy: 0.7284\n",
            "Epoch 167/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3457 - accuracy: 0.8548 - val_loss: 0.7236 - val_accuracy: 0.6927\n",
            "Epoch 168/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3432 - accuracy: 0.8520 - val_loss: 0.5020 - val_accuracy: 0.7503\n",
            "Epoch 169/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3401 - accuracy: 0.8503 - val_loss: 0.5545 - val_accuracy: 0.7257\n",
            "Epoch 170/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3433 - accuracy: 0.8479 - val_loss: 0.5270 - val_accuracy: 0.7435\n",
            "Epoch 171/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3450 - accuracy: 0.8555 - val_loss: 0.6554 - val_accuracy: 0.7064\n",
            "Epoch 172/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3413 - accuracy: 0.8558 - val_loss: 0.5058 - val_accuracy: 0.7613\n",
            "Epoch 173/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3385 - accuracy: 0.8586 - val_loss: 0.5163 - val_accuracy: 0.7545\n",
            "Epoch 174/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3341 - accuracy: 0.8582 - val_loss: 0.6381 - val_accuracy: 0.7023\n",
            "Epoch 175/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3366 - accuracy: 0.8565 - val_loss: 0.6689 - val_accuracy: 0.7092\n",
            "Epoch 176/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3344 - accuracy: 0.8568 - val_loss: 0.5238 - val_accuracy: 0.7476\n",
            "Epoch 177/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3313 - accuracy: 0.8586 - val_loss: 0.6685 - val_accuracy: 0.7133\n",
            "Epoch 178/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3315 - accuracy: 0.8620 - val_loss: 0.6437 - val_accuracy: 0.7064\n",
            "Epoch 179/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3300 - accuracy: 0.8544 - val_loss: 0.6895 - val_accuracy: 0.6900\n",
            "Epoch 180/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3296 - accuracy: 0.8565 - val_loss: 0.5331 - val_accuracy: 0.7421\n",
            "Epoch 181/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3311 - accuracy: 0.8579 - val_loss: 0.5653 - val_accuracy: 0.7407\n",
            "Epoch 182/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3249 - accuracy: 0.8596 - val_loss: 0.5243 - val_accuracy: 0.7558\n",
            "Epoch 183/200\n",
            "92/92 [==============================] - 7s 73ms/step - loss: 0.3240 - accuracy: 0.8613 - val_loss: 0.5382 - val_accuracy: 0.7503\n",
            "Epoch 184/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3316 - accuracy: 0.8565 - val_loss: 0.5330 - val_accuracy: 0.7613\n",
            "Epoch 185/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3298 - accuracy: 0.8658 - val_loss: 0.6076 - val_accuracy: 0.7202\n",
            "Epoch 186/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3277 - accuracy: 0.8579 - val_loss: 0.5761 - val_accuracy: 0.7353\n",
            "Epoch 187/200\n",
            "92/92 [==============================] - 8s 86ms/step - loss: 0.3211 - accuracy: 0.8692 - val_loss: 0.5179 - val_accuracy: 0.7545\n",
            "Epoch 188/200\n",
            "92/92 [==============================] - 8s 83ms/step - loss: 0.3268 - accuracy: 0.8555 - val_loss: 0.5731 - val_accuracy: 0.7257\n",
            "Epoch 189/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3255 - accuracy: 0.8627 - val_loss: 0.5645 - val_accuracy: 0.7380\n",
            "Epoch 190/200\n",
            "92/92 [==============================] - 7s 74ms/step - loss: 0.3196 - accuracy: 0.8634 - val_loss: 0.5776 - val_accuracy: 0.7435\n",
            "Epoch 191/200\n",
            "92/92 [==============================] - 7s 76ms/step - loss: 0.3166 - accuracy: 0.8661 - val_loss: 0.6111 - val_accuracy: 0.7092\n",
            "Epoch 192/200\n",
            "92/92 [==============================] - 7s 75ms/step - loss: 0.3250 - accuracy: 0.8661 - val_loss: 0.6068 - val_accuracy: 0.7133\n",
            "Epoch 193/200\n",
            "92/92 [==============================] - 7s 75ms/step - loss: 0.3200 - accuracy: 0.8668 - val_loss: 0.5828 - val_accuracy: 0.7270\n",
            "Epoch 194/200\n",
            "92/92 [==============================] - 7s 75ms/step - loss: 0.3142 - accuracy: 0.8644 - val_loss: 0.6090 - val_accuracy: 0.7545\n",
            "Epoch 195/200\n",
            "92/92 [==============================] - 7s 76ms/step - loss: 0.3194 - accuracy: 0.8637 - val_loss: 0.5634 - val_accuracy: 0.7517\n",
            "Epoch 196/200\n",
            "92/92 [==============================] - 7s 75ms/step - loss: 0.3132 - accuracy: 0.8665 - val_loss: 0.5951 - val_accuracy: 0.7243\n",
            "Epoch 197/200\n",
            "92/92 [==============================] - 7s 75ms/step - loss: 0.3174 - accuracy: 0.8689 - val_loss: 0.5639 - val_accuracy: 0.7435\n",
            "Epoch 198/200\n",
            "92/92 [==============================] - 7s 75ms/step - loss: 0.3100 - accuracy: 0.8733 - val_loss: 0.7588 - val_accuracy: 0.6790\n",
            "Epoch 199/200\n",
            "92/92 [==============================] - 7s 75ms/step - loss: 0.3159 - accuracy: 0.8671 - val_loss: 0.5412 - val_accuracy: 0.7462\n",
            "Epoch 200/200\n",
            "92/92 [==============================] - 7s 75ms/step - loss: 0.3088 - accuracy: 0.8651 - val_loss: 0.5134 - val_accuracy: 0.7682\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd1a712f190>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 2\n",
        "epochs = 200\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = tensorflow.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train /= 255\n",
        "#x_test /= 255\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test),shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tRlb0rkVEurE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda env:carnd-term1]",
      "language": "python",
      "name": "conda-env-carnd-term1-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "FinalCode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}