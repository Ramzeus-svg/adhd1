{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADHD Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dSsnvX3p08XMyBUugBWGEY95sQ40GaaL",
      "authorship_tag": "ABX9TyOil8taLF+cGMTSLCMQBrE/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramzeus-svg/adhd1/blob/main/ADHD_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NQqfKGnbTVA",
        "outputId": "6998828e-eb1f-4c10-d91c-2721ff7eee28"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jHPwbnZuYMcT"
      },
      "outputs": [],
      "source": [
        "from collections import Iterable\n",
        "import numpy as np\n",
        "import os, scipy.io\n",
        "\n",
        "from typing import List\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "\n",
        "# channels 3, 4, 5, 6, 13, and 14\n",
        "def load_mat(in_path: str, standardize: bool = False, channel_indices=None, window_size: int = 3) -> np.ndarray:\n",
        "    if channel_indices is None:\n",
        "        channel_indices = [3, 4, 5, 6, 13, 14]\n",
        "\n",
        "    loaded_mat = scipy.io.loadmat(in_path)\n",
        "\n",
        "    path = in_path.rsplit('/', 1)[1]  # /user/home/.../v10p.mat -> v10p.mat\n",
        "    path = path.split('.')[0]  # v10p.mat -> v10p\n",
        "\n",
        "    loaded_mat = loaded_mat[path]\n",
        "\n",
        "    loaded_mat = list(loaded_mat)\n",
        "    # loaded_mat = np.array(loaded_mat)[:-(len(loaded_mat)%window_size)]\n",
        "    # loaded_mat = np.array(loaded_mat)\n",
        "    loaded_mat = loaded_mat[:6]\n",
        "    # loaded_mat = loaded_mat[:12]\n",
        "    # loaded_mat = loaded_mat[:9285]\n",
        "\n",
        "    # print(f\"load_mat(): {path}.shape = {loaded_mat.shape}, carve = {(len(loaded_mat)%window_size)}\")\n",
        "\n",
        "    # Grab the relevant indices\n",
        "    return_list = []\n",
        "    triplets = []\n",
        "    # return_list = np.array_split(loaded_mat, window_size)\n",
        "\n",
        "    for index, row in enumerate(loaded_mat[:-window_size]):\n",
        "\n",
        "        curr = np.array([row[indice] for indice in channel_indices])\n",
        "        triplets.append(curr)\n",
        "        if len(triplets) == 3:\n",
        "            # print(f\"batch == {batch}\")\n",
        "            triplets=np.array(triplets)\n",
        "            # print(f\"triplets == {triplets}\")\n",
        "            print(f\"triplets.shape == {triplets.shape}\")\n",
        "\n",
        "            return_list.append(triplets)\n",
        "            # print(f\"return_list == {return_list}\")\n",
        "            # print(f\"return_list.shape == {return_list.shape}\")\n",
        "            triplets = []\n",
        "\n",
        "            # exit()\n",
        "\n",
        "    if standardize:\n",
        "        return_list = standardize_data(return_list)\n",
        "\n",
        "    return_list = np.array(return_list)\n",
        "    return return_list\n",
        "\n",
        "\n",
        "def standardize_data(input_iterable: Iterable) -> np.ndarray:\n",
        "    return_list = input_iterable - np.mean(input_iterable)\n",
        "    return_list /= np.std(return_list)\n",
        "    return np.array(return_list)\n",
        "\n",
        "\n",
        "# code from my github\n",
        "class ActivationLayerFactory():\n",
        "    def __init__(self, activation=tf.keras.activations.relu):\n",
        "        self.__activation_layer_counter = 0\n",
        "        self.activation = activation\n",
        "\n",
        "    def produce(self, activation=None):\n",
        "        if activation is None: activation = tf.keras.activations.relu\n",
        "        temp = tf.keras.layers.Activation(activation=activation, name='activation_' + str(self.__activation_layer_counter))\n",
        "        self.__activation_layer_counter += 1\n",
        "        return temp\n",
        "\n",
        "\n",
        "class DropoutLayerFactory():\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        self.__dropout_layer_counter = 0\n",
        "        self.rate = dropout_rate\n",
        "\n",
        "    def produce(self, rate=None):\n",
        "        if rate == None: rate = self.rate\n",
        "        temp = tf.keras.layers.Dropout(rate=rate, name='dropout' + str(self.__dropout_layer_counter))\n",
        "        self.__dropout_layer_counter += 1\n",
        "        return temp\n",
        "\n",
        "\n",
        "class BatchNormalizationLayerFactory():\n",
        "    def __init__(self):\n",
        "        self.__batchnormalization_layer_count = 0\n",
        "\n",
        "    def produce(self):\n",
        "        temp = tf.keras.layers.BatchNormalization(name='BN' + str(self.__batchnormalization_layer_count))\n",
        "        self.__batchnormalization_layer_count += 1\n",
        "        return temp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Finds reconstruction loss... should help us find out if something is similar to FOCUSING or NOT FOCUSING...\n",
        "class AutoEncoder_2D(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, encoder, decoder, loss_function=tf.keras.losses.MSE, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        self.loss_function = loss_function\n",
        "\n",
        "    def compile(self,\n",
        "                optimizer=None,\n",
        "                loss=None,\n",
        "                metrics=None,\n",
        "                loss_weights=None,\n",
        "                weighted_metrics=None,\n",
        "                run_eagerly=None,\n",
        "                steps_per_execution=None,\n",
        "                **kwargs):\n",
        "        super().compile(optimizer=optimizer)\n",
        "        self.in_optimizer = optimizer\n",
        "\n",
        "    @tf.function(experimental_compile=True)\n",
        "    def call(self, data, training=True, mask=None):\n",
        "        encoded = self.encoder(data)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "    @tf.function(experimental_compile=True)\n",
        "    def encode(self, input_data):\n",
        "        return self.encoder(input_data)\n",
        "\n",
        "    @tf.function(experimental_compile=True)\n",
        "    def decode(self, input_data):\n",
        "        return self.decoder(input_data)\n",
        "\n",
        "    @tf.function(experimental_compile=True)\n",
        "    def get_loss(self, data):\n",
        "        encoded = self.encoder(data)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return tf.reduce_mean(self.loss_function(data, decoded))\n",
        "\n",
        "    @tf.function(experimental_compile=True)\n",
        "    def train_step(self, input_data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            encoded_data = self.encoder(input_data)\n",
        "            decoded_data = self.decoder(encoded_data)\n",
        "            reconstruction_loss = self.loss_function(input_data, decoded_data)\n",
        "\n",
        "        gradients = tape.gradient(reconstruction_loss, self.trainable_variables)\n",
        "        self.in_optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "\n",
        "latent_shape = 1\n",
        "activations = tfa.activations.lisht\n",
        "output_activation = activations\n",
        "\n",
        "activation_layer_factory = ActivationLayerFactory(activation=activations)\n",
        "dropout_layer_factory = DropoutLayerFactory(dropout_rate=0.5)\n",
        "bn_layer_factory = BatchNormalizationLayerFactory()\n",
        "\n",
        "encoder_output_activation = tf.keras.activations.tanh\n",
        "decoder_output_activation = tfa.activations.lisht\n",
        "regularizer = tf.keras.regularizers.l1_l2(0.01)\n",
        "\n",
        "\n",
        "def make_encoder(in_shape):\n",
        "    encoder_input = tf.keras.Input(shape=in_shape)\n",
        "\n",
        "    x = tf.keras.layers.Convolution1D(filters=128, kernel_size=2, strides=2, kernel_regularizer=regularizer)(encoder_input)\n",
        "    x = activation_layer_factory.produce()(x)\n",
        "    x = bn_layer_factory.produce()(x)\n",
        "\n",
        "    x = tf.keras.layers.Convolution1D(filters=64, kernel_size=2, strides=2, kernel_regularizer=regularizer)(encoder_input)\n",
        "    x = activation_layer_factory.produce()(x)\n",
        "    x = bn_layer_factory.produce()(x)\n",
        "\n",
        "    x = tf.keras.layers.Convolution1D(filters=32, kernel_size=1, strides=1, kernel_regularizer=regularizer)(x)\n",
        "    x = activation_layer_factory.produce()(x)\n",
        "    x = bn_layer_factory.produce()(x)\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(latent_shape*3)(x)\n",
        "    x = activation_layer_factory.produce()(x)\n",
        "    x = bn_layer_factory.produce()(x)\n",
        "    x = tf.keras.layers.Dense(latent_shape*2)(x)\n",
        "    x = tf.keras.layers.Dense(latent_shape)(x)\n",
        "    encoder_output = encoder_output_activation(x)\n",
        "\n",
        "    encoder: tf.keras.models.Model = tf.keras.models.Model(encoder_input, encoder_output, name='encoder')\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def make_decoder(in_shape):\n",
        "    decoder_input = tf.keras.Input(shape=latent_shape)\n",
        "    x = tf.keras.layers.Reshape((1, latent_shape))(decoder_input)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=32, kernel_size=1, strides=1, kernel_regularizer=regularizer)(x)\n",
        "    x = activation_layer_factory.produce()(x)\n",
        "    x = bn_layer_factory.produce()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=1, strides=1, kernel_regularizer=regularizer)(x)\n",
        "    x = activation_layer_factory.produce()(x)\n",
        "    x = bn_layer_factory.produce()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=128, kernel_size=1, strides=1, kernel_regularizer=regularizer)(x)\n",
        "    x = tf.keras.layers.Conv1DTranspose(filters=in_shape[1], kernel_size=in_shape[0], strides=in_shape[0], kernel_regularizer=regularizer)(x)\n",
        "    decoder_output = decoder_output_activation(x)\n",
        "\n",
        "    decoder: tf.keras.Model = tf.keras.Model(inputs=decoder_input, outputs=decoder_output, name=\"decoder\")\n",
        "    return decoder\n"
      ],
      "metadata": {
        "id": "fTeqoYv6bdjr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "\n",
        "# os.system(\"python -m pip install -U sklearn tensorflow-addons tensorflow-probability\")\n",
        "import string, scipy.io\n",
        "\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "convert_model = False\n",
        "if convert_model:\n",
        "    encoder_converter = tf.lite.TFLiteConverter.from_saved_model(\"/content/drive/MyDrive/saved_models/encoder_2D\")\n",
        "    decoder_converter = tf.lite.TFLiteConverter.from_saved_model(\"/content/drive/MyDrive/saved_models/decoder_2D\")\n",
        "    encoder_model = encoder_converter.convert()\n",
        "    decoder_model = decoder_converter.convert()\n",
        "    with open('/content/drive/MyDrive/saved_models/encoder.tflite', 'wb') as f:\n",
        "        f.write(encoder_model)\n",
        "    with open('/content/drive/MyDrive/saved_models/decoder.tflite', 'wb') as f:\n",
        "        f.write(decoder_model)\n",
        "    print('\\nFinished saving models!')\n",
        "    exit()\n",
        "\n",
        "adhd_path = \"/content/drive/MyDrive/data/adhd/\"\n",
        "control_part1_path = \"/content/drive/MyDrive/data/Control_part1/\"\n",
        "\n",
        "# Get the file paths for adhd_part1_paths and control_part1_paths\n",
        "adhd_paths = [adhd_path+ path for path in os.listdir(adhd_path)]\n",
        "control_part1_paths = [control_part1_path + path for path in os.listdir(control_part1_path)]\n",
        "\n",
        "# Opens the files into numpy arrays\n",
        "standardize = False\n",
        "channel_indices = [3, 4, 5, 6, 13, 14]\n",
        "window_size = 3  # gives us a 2D array\n",
        "batch_size = 20\n",
        "\n",
        "# Quick and dirty copy-pasted functions\n",
        "def load_adhd(index):\n",
        "    return load_mat(adhd_paths[index], standardize=standardize, channel_indices=channel_indices, window_size=window_size)\n",
        "\n",
        "\n",
        "def load_control_part1(index):\n",
        "    return load_mat(control_part1_paths[index], standardize=standardize, channel_indices=channel_indices, window_size=window_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Multiprocessing for loading and processing the data faster\n",
        "adhd_data = np.squeeze(np.array(mp.Pool(mp.cpu_count()).map(load_adhd, range(len(adhd_paths))), dtype=np.float32))\n",
        "control_part1_data = np.squeeze(np.array(mp.Pool(mp.cpu_count()).map(load_control_part1, range(len(control_part1_paths))), dtype=np.float32))\n",
        "\n",
        "flat_data = adhd_data.flatten()\n",
        "mean, std_dev = np.median(flat_data), np.std(flat_data)\n",
        "for index, row in enumerate(adhd_data):\n",
        "    adhd_data[index] = (row - mean) / std_dev\n",
        "\n",
        "flat_data = control_part1_data.flatten()\n",
        "mean, std_dev = np.median(flat_data), np.std(flat_data)\n",
        "for index, row in enumerate(control_part1_data):\n",
        "    control_part1_data[index] = (row - mean) / std_dev\n",
        "\n",
        "# Print out the input_data for debugging\n",
        "print('\\n\\n\\n')\n",
        "for batch, line in enumerate(adhd_data):\n",
        "    print(f\"adhd_data #{batch}: {line}, shape: {line.shape}\")\n",
        "    break\n",
        "for batch, line in enumerate(control_part1_data):\n",
        "    print(f\"control_part1_data #{batch}: {line}, shape: {line.shape}\")\n",
        "    break\n",
        "\n",
        "print('\\n\\n')\n",
        "print(f\"adhd_data.shape: {adhd_data.shape}\")\n",
        "print(f\"control_part1_data.shape: {control_part1_data.shape}\")\n",
        "\n",
        "#########################\n",
        "## Defining the model\n",
        "#########################\n",
        "in_shape = (window_size, len(channel_indices))\n",
        "\n",
        "encoder = make_encoder(in_shape=in_shape)\n",
        "decoder = make_decoder(in_shape=in_shape)\n",
        "\n",
        "encoder.summary()\n",
        "decoder.summary()\n",
        "\n",
        "lr_schedule = tfa.optimizers.ExponentialCyclicalLearningRate(1e-4, 1e-3, step_size=2000, gamma=0.96)\n",
        "in_optimizer = tfa.optimizers.LAMB(learning_rate=lr_schedule)\n",
        "\n",
        "AE = AutoEncoder_2D(encoder=encoder, decoder=decoder)\n",
        "AE.compile(in_optimizer)\n",
        "\n",
        "adhd_data = tf.convert_to_tensor(adhd_data, dtype=tf.float32)\n",
        "\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def train_loop(data, batch_size=3):\n",
        "    for batch in range(0, len(data), batch_size):\n",
        "        start = batch * batch_size\n",
        "        each = data[start:start + batch_size]\n",
        "        AE.train_step(each)\n",
        "\n",
        "\n",
        "print('Training...')\n",
        "epochs = 10000  # 00\n",
        "for epoch in range(epochs):\n",
        "    train_loop(adhd_data, batch_size=batch_size)\n",
        "    if epoch % 1000 == 0:\n",
        "        adhd_data = adhd_data[0:batch_size]\n",
        "        adhd_loss_value = AE.get_loss(adhd_data)\n",
        "        control_data = control_part1_data[0:batch_size]\n",
        "        control_loss_value = AE.get_loss(control_data)\n",
        "        print(\n",
        "            \"\\n\",\n",
        "            f\"Epoch #{epoch}:\",\n",
        "            f\"\\n target: \\n {adhd_data[0]}\",\n",
        "            f\"\\n prediction on adhd: \\n {AE.call(adhd_data[0:1])}\",\n",
        "            f\"\\n prediction on control: \\n {AE.call(control_data[0:1])}\",\n",
        "            f\" \\t reconstruction loss on ADHD == {adhd_loss_value:0.2f}\",\n",
        "            f\" \\t reconstruction loss on control == {control_loss_value:0.2f}\",\n",
        "            sep='', end='', flush=True,\n",
        "        )\n",
        "\n",
        "        if adhd_loss_value < 0.4 and control_loss_value > adhd_loss_value * 2.5:\n",
        "            print('\\n\\n')\n",
        "            break\n",
        "\n",
        "print('\\nFinished training!')\n",
        "\n",
        "# Saves models\n",
        "print('\\nSaving models...')\n",
        "AE.encoder.save(\"/content/drive/MyDrive/saved_models/encoder_2D\")\n",
        "AE.decoder.save(\"/content/drive/MyDrive/saved_models/decoder_2D\")\n",
        "encoder_converter = tf.lite.TFLiteConverter.from_saved_model(\"/content/drive/MyDrive/saved_models/encoder_2D\")\n",
        "decoder_converter = tf.lite.TFLiteConverter.from_saved_model(\"/content/drive/MyDrive/saved_models/decoder_2D\")\n",
        "encoder_model = encoder_converter.convert()\n",
        "decoder_model = decoder_converter.convert()\n",
        "with open('/content/drive/MyDrive/saved_models/encoder.tflite', 'wb') as f:\n",
        "    f.write(encoder_model)\n",
        "with open('/content/drive/MyDrive/saved_models/decoder.tflite', 'wb') as f:\n",
        "    f.write(decoder_model)\n",
        "\n",
        "# Save losses per triplet line\n",
        "adhd_f = open(\"/content/drive/MyDrive/data/adhd_reconstructed.txt\", \"w\")\n",
        "control_f = open(\"/content/drive/MyDrive/data/control_reconstructed.txt\", \"w\")\n",
        "test_f = open(\"/content/drive/MyDrive/data/original_lines.txt\", \"w\")\n",
        "contest_f = open(\"/content/drive/MyDrive/data/original_lines_control.txt\", \"w\")\n",
        "ratio_f = open(\"/content/drive/MyDrive/data/ratios.txt\", \"w\")\n",
        "\n",
        "adhd_write_lines = []\n",
        "control_write_lines = []\n",
        "\n",
        "for index, row in enumerate(adhd_data):\n",
        "\n",
        "    control_data = control_part1_data[index:index + 3]\n",
        "    control_loss_value = np.array(AE.get_loss(control_data))\n",
        "    control_write_lines.append(str(control_loss_value))\n",
        "    control_write_lines.append(\"\\n\")\n",
        "\n",
        "    adhd_data = adhd_data[index:index + 3]\n",
        "    adhd_loss_value = np.array(AE.get_loss(adhd_data))\n",
        "    adhd_write_lines.append(str(adhd_loss_value))\n",
        "    adhd_write_lines.append(\"\\n\")\n",
        "\n",
        "    ratio_f.write(f\"{control_loss_value / adhd_loss_value:1.2f}\\n\")\n",
        "\n",
        "    test_data = list(np.squeeze(np.array(adhd_data)))\n",
        "    curr_line = []\n",
        "    for elem in test_data:\n",
        "        for more in elem:\n",
        "            if type(more) != np.float32 and len(more) > 1:\n",
        "                test_f.write(str(more) + \"\\n\")\n",
        "\n",
        "    cont_data = list(np.squeeze(np.array(control_data)))\n",
        "    curr_line = []\n",
        "    for elem in cont_data:\n",
        "        for more in elem:\n",
        "            if type(more) != np.float32 and len(more) > 1:\n",
        "                contest_f.write(str(more) + \"\\n\")\n",
        "\n",
        "adhd_f.writelines(adhd_write_lines)\n",
        "control_f.writelines(control_write_lines)\n"
      ],
      "metadata": {
        "id": "gUXy-WNwbxfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069a0337-9718-437f-8062-8613e86963fa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "triplets.shape == (3, 6)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adhd_data #0: [[-0.30698407 -0.2398313  -0.5218729  -0.17075989 -0.5218729   0.39332333]\n",
            " [ 0.5026864   1.2413667   0.6753649   1.0283966   0.4643134   0.04029166]\n",
            " [-0.50844234 -0.03069841  0.04029166 -0.10168847 -0.10168847 -0.17075989]], shape: (3, 6)\n",
            "control_part1_data #0: [[-0.42080927 -0.49094415 -0.1779031  -0.1779031  -0.0513182   0.51489264]\n",
            " [-0.360938   -0.49094415 -0.0513182  -0.23948495  0.3267259   0.70476997]\n",
            " [-0.05987124  0.0752667   0.200141    0.3267259   0.4516002   0.8929367 ]], shape: (3, 6)\n",
            "\n",
            "\n",
            "\n",
            "adhd_data.shape: (30, 3, 6)\n",
            "control_part1_data.shape: (30, 3, 6)\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 3, 6)]            0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 1, 64)             832       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1, 64)             0         \n",
            "                                                                 \n",
            " BN1 (BatchNormalization)    (None, 1, 64)             256       \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 1, 32)             2080      \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 1, 32)             0         \n",
            "                                                                 \n",
            " BN2 (BatchNormalization)    (None, 1, 32)             128       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 99        \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 3)                 0         \n",
            "                                                                 \n",
            " BN3 (BatchNormalization)    (None, 3)                 12        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 8         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            " tf.math.tanh (TFOpLambda)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,418\n",
            "Trainable params: 3,220\n",
            "Non-trainable params: 198\n",
            "_________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 1, 1)         0           ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 1, 32)        64          ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 1, 32)        0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " BN4 (BatchNormalization)       (None, 1, 32)        128         ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 1, 64)        2112        ['BN4[0][0]']                    \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 1, 64)        0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " BN5 (BatchNormalization)       (None, 1, 64)        256         ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 1, 128)       8320        ['BN5[0][0]']                    \n",
            "                                                                                                  \n",
            " conv1d_transpose (Conv1DTransp  (None, 3, 6)        2310        ['conv1d_5[0][0]']               \n",
            " ose)                                                                                             \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor (TFOpLamb  (None, 3, 6)        0           ['conv1d_transpose[0][0]']       \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.math.tanh_1 (TFOpLambda)    (None, 3, 6)         0           ['tf.convert_to_tensor[0][0]']   \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None, 3, 6)         0           ['tf.convert_to_tensor[0][0]',   \n",
            "                                                                  'tf.math.tanh_1[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 13,190\n",
            "Trainable params: 12,998\n",
            "Non-trainable params: 192\n",
            "__________________________________________________________________________________________________\n",
            "Training...\n",
            "\n",
            "Epoch #0:\n",
            " target: \n",
            " [[-0.30698407 -0.2398313  -0.5218729  -0.17075989 -0.5218729   0.39332333]\n",
            " [ 0.5026864   1.2413667   0.6753649   1.0283966   0.4643134   0.04029166]\n",
            " [-0.50844234 -0.03069841  0.04029166 -0.10168847 -0.10168847 -0.17075989]]\n",
            " prediction on adhd: \n",
            " [[[6.27516329e-06 1.02083693e-04 3.04091955e-05 4.50804473e-05\n",
            "   1.10176734e-04 7.95786764e-06]\n",
            "  [2.11390125e-05 2.29192410e-05 4.80657764e-05 3.74132082e-06\n",
            "   7.26885673e-06 3.74076808e-05]\n",
            "  [2.10126818e-05 4.42805504e-05 1.34356451e-05 9.03810271e-09\n",
            "   4.19784101e-06 5.77754281e-06]]]\n",
            " prediction on control: \n",
            " [[[3.6487003e-05 6.0102710e-04 1.9150171e-04 2.7780811e-04 8.2193565e-04\n",
            "   4.8057060e-05]\n",
            "  [1.4979167e-04 1.6454025e-04 3.6829597e-04 2.8469931e-05 4.6731071e-05\n",
            "   2.6952612e-04]\n",
            "  [1.5315376e-04 3.2801600e-04 9.3733892e-05 7.0731903e-07 3.5073426e-05\n",
            "   5.4978031e-05]]] \t reconstruction loss on ADHD == 1.15 \t reconstruction loss on control == 0.88\n",
            "Epoch #1000:\n",
            " target: \n",
            " [[-0.30698407 -0.2398313  -0.5218729  -0.17075989 -0.5218729   0.39332333]\n",
            " [ 0.5026864   1.2413667   0.6753649   1.0283966   0.4643134   0.04029166]\n",
            " [-0.50844234 -0.03069841  0.04029166 -0.10168847 -0.10168847 -0.17075989]]\n",
            " prediction on adhd: \n",
            " [[[2.2898249e-03 1.1479245e-02 5.1241680e-03 6.6726250e-03 2.9507992e-04\n",
            "   2.8333307e-04]\n",
            "  [1.1310444e-05 1.6354912e-05 6.2677004e-06 8.2272692e-08 1.1309627e-05\n",
            "   8.5299320e-07]\n",
            "  [8.7266490e-06 3.8125628e-04 1.3184127e-04 7.2417001e-04 1.6433304e-07\n",
            "   1.4531040e-05]]]\n",
            " prediction on control: \n",
            " [[[7.7985413e-04 3.9522247e-03 1.7462451e-03 2.2796504e-03 9.4997762e-05\n",
            "   9.8707220e-05]\n",
            "  [3.7960872e-06 5.5156675e-06 1.5400693e-06 6.0646983e-08 4.1527555e-06\n",
            "   3.3312523e-07]\n",
            "  [2.8026273e-06 1.2586830e-04 4.4402423e-05 2.4838810e-04 1.2980654e-07\n",
            "   6.2917024e-06]]] \t reconstruction loss on ADHD == 1.00 \t reconstruction loss on control == 0.83\n",
            "Epoch #2000:\n",
            " target: \n",
            " [[-0.30698407 -0.2398313  -0.5218729  -0.17075989 -0.5218729   0.39332333]\n",
            " [ 0.5026864   1.2413667   0.6753649   1.0283966   0.4643134   0.04029166]\n",
            " [-0.50844234 -0.03069841  0.04029166 -0.10168847 -0.10168847 -0.17075989]]\n",
            " prediction on adhd: \n",
            " [[[4.1972502e-04 3.0931947e-03 5.5297341e-02 1.7488123e-03 1.5936403e-04\n",
            "   9.1249710e-03]\n",
            "  [3.9473248e-01 9.2278010e-01 2.4418592e-01 7.6869041e-01 1.0729545e-01\n",
            "   2.8874132e-01]\n",
            "  [6.3128091e-06 1.9147307e-02 3.5878170e-02 4.2291600e-03 7.9616357e-06\n",
            "   7.8953692e-04]]]\n",
            " prediction on control: \n",
            " [[[8.5553447e-06 4.8663776e-05 4.2948304e-04 2.4864734e-05 2.0999182e-06\n",
            "   5.2464067e-05]\n",
            "  [2.9584111e-03 8.4140403e-03 1.7709180e-03 6.6199820e-03 7.3595101e-04\n",
            "   2.1034239e-03]\n",
            "  [1.4178067e-08 1.2775902e-04 2.3547637e-04 3.3792403e-05 2.7073173e-09\n",
            "   2.6218802e-06]]] \t reconstruction loss on ADHD == 0.71 \t reconstruction loss on control == 0.77\n",
            "Epoch #3000:\n",
            " target: \n",
            " [[-0.30698407 -0.2398313  -0.5218729  -0.17075989 -0.5218729   0.39332333]\n",
            " [ 0.5026864   1.2413667   0.6753649   1.0283966   0.4643134   0.04029166]\n",
            " [-0.50844234 -0.03069841  0.04029166 -0.10168847 -0.10168847 -0.17075989]]\n",
            " prediction on adhd: \n",
            " [[[1.6779356e-07 9.7737267e-09 5.1882101e-04 5.2501441e-06 3.5256809e-07\n",
            "   3.0325668e-02]\n",
            "  [5.7171857e-01 1.0399401e+00 6.7844516e-01 1.0302091e+00 4.2939821e-01\n",
            "   3.5278872e-01]\n",
            "  [2.0936152e-08 3.2738302e-02 1.6374016e-02 1.1683517e-04 3.5020161e-08\n",
            "   1.6429334e-07]]]\n",
            " prediction on control: \n",
            " [[[1.5272048e-03 2.3717186e-03 2.0327356e-03 1.9704700e-03 4.5018535e-07\n",
            "   1.6357049e-07]\n",
            "  [8.0518729e-09 1.9147741e-09 4.4094662e-07 1.1069663e-07 1.5026903e-07\n",
            "   1.3708272e-08]\n",
            "  [2.4808939e-08 8.7160075e-08 4.4617099e-10 2.5372745e-07 9.2518405e-08\n",
            "   6.3886461e-07]]] \t reconstruction loss on ADHD == 0.18 \t reconstruction loss on control == 0.73\n",
            "\n",
            "\n",
            "\n",
            "Finished training!\n",
            "\n",
            "Saving models...\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/saved_models/encoder_2D/assets\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/saved_models/decoder_2D/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # to work with arrays\n",
        "import matplotlib.pyplot as plt # to plot dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = adhd_path[0:-1]\n",
        "y = control_part1_path[0 : -1] \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/2,random_state=1)\n",
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "67l_pLEqngrf",
        "outputId": "190e2873-ab61-45a0-b5c8-1b89915d65a5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-6b992fedf920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_part1_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2417\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    332\u001b[0m         raise ValueError(\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [32, 41]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_train, y_train, color = 'red') #plotting the training values (experiance) , with training values salary as red plots \n",
        "plt.plot(X_train, Model.predict(X_train), color = 'blue') # plot the preidctaing line to compare the predictaing values with real \n",
        "plt.title('ADHD (Training set)') \n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "iqaEsV7rq28S",
        "outputId": "915dfcb0-90f4-412a-93da-68d061e81490"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-480d1c67aef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#plotting the training values (experiance) , with training values salary as red plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# plot the preidctaing line to compare the predictaing values with real\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ADHD (Training set)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    }
  ]
}